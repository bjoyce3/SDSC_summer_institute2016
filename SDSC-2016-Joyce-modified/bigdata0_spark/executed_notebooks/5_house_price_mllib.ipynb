{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load house prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_RDD = sc.textFile(\n",
    "    \"file:///oasis/scratch/comet/zonca/temp_project/houses_1mil.txt\",\n",
    "    minPartitions=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mapper_parse_lines(line):\n",
    "    \"\"\"Parse line into (neighborhoood, price) pair\"\"\"\n",
    "    words = line.split()\n",
    "    return (words[1], float(words[2]), int(words[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "house_prices_RDD = text_RDD.map(mapper_parse_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Port', 93000.0, 2),\n",
       " (u'Hilltop', 293000.0, 2),\n",
       " (u'Hilltop', 605000.0, 4),\n",
       " (u'Hilltop', 290000.0, 2),\n",
       " (u'Downtown', 418000.0, 4)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_prices_RDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "house_prices_df = sqlContext.createDataFrame(house_prices_RDD, [\"neighborhood\", \"price\", \"bedrooms\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[neighborhood: string, price: double, bedrooms: bigint]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_prices_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- neighborhood: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- bedrooms: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "house_prices_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"neighborhood\",\n",
    "                              outputCol=\"neighborhood_index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "house_prices_df_indexed = stringIndexer.fit(house_prices_df).transform(house_prices_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+--------+------------------+\n",
      "|neighborhood|   price|bedrooms|neighborhood_index|\n",
      "+------------+--------+--------+------------------+\n",
      "|        Port| 93000.0|       2|               0.0|\n",
      "|     Hilltop|293000.0|       2|               1.0|\n",
      "|     Hilltop|605000.0|       4|               1.0|\n",
      "|     Hilltop|290000.0|       2|               1.0|\n",
      "|    Downtown|418000.0|       4|               2.0|\n",
      "|     Hilltop|271000.0|       2|               1.0|\n",
      "|     Hilltop|296000.0|       2|               1.0|\n",
      "|        Port|236000.0|       4|               0.0|\n",
      "|    Downtown|219000.0|       2|               2.0|\n",
      "|    Downtown|289000.0|       3|               2.0|\n",
      "|     Hilltop|404000.0|       3|               1.0|\n",
      "|        Port|149000.0|       3|               0.0|\n",
      "|        Port|157000.0|       3|               0.0|\n",
      "|     Hilltop|552000.0|       4|               1.0|\n",
      "|    Downtown|217000.0|       2|               2.0|\n",
      "|    Downtown|438000.0|       4|               2.0|\n",
      "|    Downtown|299000.0|       3|               2.0|\n",
      "|     Hilltop|601000.0|       4|               1.0|\n",
      "|        Port|230000.0|       4|               0.0|\n",
      "|        Port|241000.0|       4|               0.0|\n",
      "+------------+--------+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "house_prices_df_indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.mllib.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_features(row):\n",
    "    return Row(label=row.price,\n",
    "               features=Vectors.dense([row.bedrooms, row.neighborhood_index])\n",
    "              )\n",
    "    \n",
    "house_prices_features = sqlContext.createDataFrame(\n",
    "    house_prices_df_indexed.map(create_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "| features|   label|\n",
      "+---------+--------+\n",
      "|[2.0,0.0]| 93000.0|\n",
      "|[2.0,1.0]|293000.0|\n",
      "|[4.0,1.0]|605000.0|\n",
      "|[2.0,1.0]|290000.0|\n",
      "|[4.0,2.0]|418000.0|\n",
      "|[2.0,1.0]|271000.0|\n",
      "|[2.0,1.0]|296000.0|\n",
      "|[4.0,0.0]|236000.0|\n",
      "|[2.0,2.0]|219000.0|\n",
      "|[3.0,2.0]|289000.0|\n",
      "|[3.0,1.0]|404000.0|\n",
      "|[3.0,0.0]|149000.0|\n",
      "|[3.0,0.0]|157000.0|\n",
      "|[4.0,1.0]|552000.0|\n",
      "|[2.0,2.0]|217000.0|\n",
      "|[4.0,2.0]|438000.0|\n",
      "|[3.0,2.0]|299000.0|\n",
      "|[4.0,1.0]|601000.0|\n",
      "|[4.0,0.0]|230000.0|\n",
      "|[4.0,0.0]|241000.0|\n",
      "+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "house_prices_features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "house_prices_features = house_prices_features.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit a Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RandomForestRegressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression parameters:\n",
      "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. (default: False)\n",
      "checkpointInterval: checkpoint interval (>= 1) (default: 10)\n",
      "featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2 (default: auto, current: all)\n",
      "featuresCol: features column name (default: features)\n",
      "impurity: Criterion used for information gain calculation (case-insensitive). Supported options: variance (default: variance)\n",
      "labelCol: label column name (default: label)\n",
      "maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\n",
      "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5)\n",
      "maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. (default: 256)\n",
      "minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n",
      "minInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\n",
      "numTrees: Number of trees to train (>= 1) (default: 20, current: 10)\n",
      "predictionCol: prediction column name (default: prediction)\n",
      "seed: random seed (default: None)\n",
      "subsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (undefined)\n"
     ]
    }
   ],
   "source": [
    "regr = RandomForestRegressor(numTrees=10, featureSubsetStrategy=\"all\")\n",
    "# Print out the parameters, documentation, and any default values.\n",
    "print(\"LogisticRegression parameters:\\n\" + regr.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Learn a RandomForestRegressor model. This uses the parameters stored in regr.\n",
    "model = regr.fit(house_prices_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = model.transform(house_prices_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+\n",
      "| features|   label|        prediction|\n",
      "+---------+--------+------------------+\n",
      "|[2.0,0.0]| 93000.0|104511.63102449726|\n",
      "|[2.0,1.0]|293000.0|279509.60688933445|\n",
      "|[4.0,1.0]|605000.0|  599573.747561463|\n",
      "|[2.0,1.0]|290000.0|279509.60688933445|\n",
      "|[4.0,2.0]|418000.0| 449537.9869135598|\n",
      "|[2.0,1.0]|271000.0|279509.60688933445|\n",
      "|[2.0,1.0]|296000.0|279509.60688933445|\n",
      "|[4.0,0.0]|236000.0| 224458.9185392844|\n",
      "|[2.0,2.0]|219000.0|209501.75616016105|\n",
      "|[3.0,2.0]|289000.0|299523.14600544784|\n",
      "|[3.0,1.0]|404000.0| 399542.4684480513|\n",
      "|[3.0,0.0]|149000.0|149472.14182606322|\n",
      "|[3.0,0.0]|157000.0|149472.14182606322|\n",
      "|[4.0,1.0]|552000.0|  599573.747561463|\n",
      "|[2.0,2.0]|217000.0|209501.75616016105|\n",
      "|[4.0,2.0]|438000.0| 449537.9869135598|\n",
      "|[3.0,2.0]|299000.0|299523.14600544784|\n",
      "|[4.0,1.0]|601000.0|  599573.747561463|\n",
      "|[4.0,0.0]|230000.0| 224458.9185392844|\n",
      "|[4.0,0.0]|241000.0| 224458.9185392844|\n",
      "+---------+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_squared_error = output.withColumn(\n",
    "    \"squared_error\",\n",
    "    pow(output.label - output.prediction,2)\n",
    "    ).agg({\"squared_error\":\"mean\"}).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(avg(squared_error)=282413983.0559324)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16805.177269399224"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.sqrt(mean_squared_error[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
